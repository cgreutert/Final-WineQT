---
title: "Predicting Vinho Verde Wine Quality"
author: "Carly Greutert"
output: 
  html_document:
    toc: TRUE
    theme: cosmo
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r collapse = TRUE}
library(vip)
library(knitr)
library(rmdformats)
library(tidymodels)
library(generics)
library(glmnet)
library(ggplot2)
library(discrim)
library(corrr)
library(klaR)
library(caret)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(corrplot)
library(ggthemes)
library(cli)
library(recipes)
library(pROC)
library(MASS)
library(poissonreg)
library(naivebayes)
library(readxl)
library(janitor)
library(kernlab)
library(kknn)
tidymodels_prefer()
```

# Introduction

Across age groups and borders, the grape-derived delicacy, wine, has been enjoyed (responsibly) by several, but what makes a specific wine "good" or "bad"? Wine enthusiasts often rely on reviews by connoisseurs and other 'expert' groups who classify the quality of wine before they choose from an overwhelming selection. The quality score given by experts is mostly based on the taster's experience, which can be incredibly subjective. I seek to use objective characteristics of wine to predict its quality score. The data set I will be working with for this project was originally created by the UCI Machine Learning Repository and published by Kaggle (https://www.kaggle.com/datasets/yasserh/wine-quality-dataset). The data collected describes the wine quality of the red and white variants of the north west located Portuguese "Vinho Verde" wine using a quality scale from 1 to 10.

## Loading and Cleaning Data
```{r collapse = TRUE}
wine <- read_csv('C:\\Program Files\\Git\\tmp\\131-finalproj\\WineQT.csv')
wine <- wine %>% clean_names()
wine <- wine %>% select(-id)
wine$quality <- as.factor(wine$quality)
head(wine)
nrow(wine)
is.null(wine)
```

## An Overview of the Data Set 

This data set has 1143 observations and 11 predictors (fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, alcohol) based on physicochemical tests to give a quality score between 0 and 10. Note there is an 'id' variable that simply counts the number of observations there are. Since this is not relevant to our precdiction, I elected to remove it from the data set. The predictor variables are all numeric and can take any real, positive value. Quality, our response variable, is a whole integer between 0 and 10. Note that I converted 'quality' into a factor to reflect that it is a categorical value, not numeric. There are no missing values in my data set, so all observations are used for the model.

## Exploratory Research Questions   

I am interested in predicting the wine quality score. Some questions I hope to explore related to this prediction include the following:                                                      

- Which predictors are most important in predicting quality?                                   
- Which set of predictors produce the most accurate results?                                   
- Are all of our predictors relevant?                                                          
- Do there exist any collinear relationships between predictors influencing the data?          
- Which model/approach yields the highest accuracy of prediction?

I intend to use a classification model, as opposed to a regression approach. 

# Exploratory Data Analysis (EDA)

## Predictor Summary Statistics
```{r collapse = TRUE}
summary(wine)
```

## Univariate Analysis
```{r collapse = TRUE}
wine %>% ggplot(aes(x = quality))+ geom_bar(stat = "count")
```

It appears that this dataset only consists of wine with a quality between 3 and 8. This shows we just took an average sample of wine, there are not really outliers. Furthermore, the quality scores appear to follow a normal distribution.

## Multivariate Analysis
```{r collapse = TRUE}
wine %>% select(where(is.numeric)) %>% cor() %>% corrplot(type="lower")
```

It appears the following pairings are highly correlated: fixed acidity/citric acid, volatile acidity/citric acid, fixed acidity/density, fixed acidity/pH, citric acid/pH, free sulfur dioxide/total sulfur dioxide, and density/alcohol.

## Bivariate Analysis
```{r collapse = TRUE}
wine %>% 
  ggplot(aes(x = alcohol, y = quality)) +
  geom_boxplot() +
  xlab("Alcohol Content") +
  ylab("Quality Score") +
  coord_flip()
```

My initial assumption is the higher the alcohol content, the higher the quality score. 


```{r collapse = TRUE}
wine %>% 
  ggplot(aes(x = quality, y = fixed_acidity)) +
  geom_boxplot()
```

My initial assumption is that fixed acidity has some outliers when the quality score is 5 or 6, but fixed acidity does not greatly change its quality score.

# Cross Validation
```{r collapse = TRUE}
set.seed(1212)
wine_split <- initial_split(wine, prop = 0.80, strata = 'quality')
wine_train <- training(wine_split)
wine_test <- testing(wine_split)
dim(wine_train)
dim(wine_test)
wine_folds <- vfold_cv(wine_train, strata = quality, v = 5, repeats=5)
wine_recipe <- recipe(quality ~ fixed_acidity + volatile_acidity + citric_acid + residual_sugar + chlorides + free_sulfur_dioxide + total_sulfur_dioxide + density + p_h + sulphates + alcohol, wine_train) %>% 
                  step_center(all_predictors()) %>% 
                  step_scale(all_predictors())
```

The first step to building our model is setting our seed to save results. Then, I split the data into a testing and training set, with the proportion set to 0.80, stratifying on the quality variable. Then, I checked the dimensions of each data set to verify it was split correctly. I decided to employ cross-validation by folding the data into 5 partitions. I decided not to do 10 splits since there are only 1143 observations and I do not want to overfit the data. I also conducted five repeats. Then, I set up a recipe to predict to quality and centered and scaled all predictors.

# Model Fitting

Now that we've split our data and set up our recipe, I will fit seven different types of models to predict quality at the highest degree of accuracy. 

## Naive Bayes Model
```{r collapse=TRUE, message=FALSE, warning=FALSE}
naive_mod <- 
  naive_Bayes()%>%
    set_mode("classification") %>% 
    set_engine("klaR") %>%
    set_args(usekernel = FALSE)

naive_wf <- workflow() %>% 
  add_model(naive_mod) %>% 
  add_recipe(wine_recipe)

naive_fit <- fit_resamples(naive_wf, wine_folds)

collect_metrics(naive_fit)
```
The first model I am fitting is a Naive Bayes model. I fit it to the resamples and the roc_auc value is 0.706 and the accuracy value is 0.507 with standard errors of 0.00798 and 0.0122, respectively. 

## Support Vector Machine (SVM) with Linear Kernel Model
```{r collapse=TRUE, message=FALSE, warning=FALSE}
svm_linear_mod <- svm_poly(degree = 1) %>%
  set_mode("classification") %>%
  set_engine("kernlab", scaled = FALSE)

svm_linear_wf <- workflow() %>%
  add_model(svm_linear_mod %>% set_args(cost = tune())) %>%
  add_recipe(wine_recipe)

svm_grid <- grid_regular(cost(), levels = 10)

svm_tune <- tune_grid(
  svm_linear_wf, 
  resamples = wine_folds, 
  grid = svm_grid
)

autoplot(svm_tune)

show_best(svm_tune)
```

The next model I decided to fit is a Support Vector Classifier with Linear Kernel. I decided to set up a grid to tune the cost parameter. It looks like the higher the cost, the higher accuracy. However, in the case of the roc_auc values, its performance is more varied, with it peaking at a lower cost. It appears that the best cost is 0.000977, yielding a value of 0.692 and a standard error of 0.00845. 

## SVM with Non-Linear Kernel Model
```{r collapse=TRUE, message=FALSE, warning=FALSE}
svm_nonlinear_mod <- svm_rbf() %>%
  set_mode("classification") %>%
  set_engine("kernlab")

svm_nonlinear_wf <- workflow() %>%
  add_model(svm_nonlinear_mod %>% set_args(cost = tune())) %>%
  add_recipe(wine_recipe)

svm_gridn <- grid_regular(cost(), levels = 10)

svm_tunen <- tune_grid(
  svm_nonlinear_wf, 
  resamples = wine_folds, 
  grid = svm_gridn
)

autoplot(svm_tunen)
show_best(svm_tunen)
```

I decided to see how the performance changes with a non-linear kernel for the SVM model. I am still tuning with the cost parameter. Similar to the linear kernel, as cost increases, accuracy does as well. However, it differs in the roc_auc case since as cost increases, the roc_auc increases as well. It appears that the best performing model has a cost value of 0.0312, yielding a roc_auc of 0.718 and a standard error of 0.00812. This is slightly better than the linear case, but I will fit more models with a different approach. 


## K NNearest Neighbors Model
```{r collapse=TRUE, message=FALSE, warning=FALSE}
knn_mod <- 
  nearest_neighbor(neighbors = tune())%>%
    set_mode("classification") %>% 
    set_engine("kknn")

knn_wf <- workflow() %>% 
  add_model(knn_mod) %>% 
  add_recipe(wine_recipe)

knn_grid <- grid_regular(neighbors(), levels = 10)

knn_tune <- tune_grid(
  knn_wf, 
  resamples = wine_folds, 
  grid = knn_grid
)

autoplot(knn_tune)
show_best(knn_tune)
```

For this model, I decided to employ the K-Nearest Neighbors approach. The parameter I decided to tune was the number of nearest neighbors. In the accuracy case, the value stays pretty steady between 0 and 6 nearest neighbors, then drops before increasing quickly. The roc_auc case has more of a steady increase with a soft curve upward. The best tuned model has 10 neighbors with an roc_auc value of 0.659 and standard error of 0.00717.  

## Random Forest Model
```{r collapse=TRUE, message=FALSE, warning=FALSE}
forest_model <- 
  rand_forest(trees = tune(),
              min_n = tune(),
              mtry = tune()) %>%
              set_mode("classification") %>% 
              set_engine("ranger", importance = 'impurity')

forest_wf <- workflow() %>% 
  add_model(forest_model) %>% 
  add_recipe(wine_recipe)

param_gridf <- grid_regular(min_n(range = c(1,10)), trees(range = c(1,50)), mtry(range = c(1,11)), levels= 5)

tune_resf <- tune_grid(
  forest_wf, 
  resamples = wine_folds, 
  grid = param_gridf
)

autoplot(tune_resf)
show_best(tune_resf)
```

Now, I will fit a forest model to the training data. The parameters I am tuning are the minimal node size (min_n), the number of predictors in each fold (mtry), and the number of trees. It looks like the higher number of trees, the better performance for roc_auc and accuracy. In both cases, the fewer number of predictors (mtry), the better. For minimal node size, it varies without much change in performance, but it appears the higher number the better. The best forest model has a 3 mtry value, 50 trees, and 10 minimal nodes to yield a roc_auc value of 0.782 and a standard error of 0.00788. 

## Bagged Model
```{r collapse=TRUE, message=FALSE, warning=FALSE}
bagging_mod <- rand_forest(mtry = 11, min_n = tune(), trees = tune()) %>%
  set_engine("randomForest", importance = TRUE) %>%
  set_mode("classification")

bagging_wf <- workflow() %>%
  add_model(bagging_mod) %>%
  add_recipe(wine_recipe)

param_gridbag <- grid_regular(min_n(range = c(1,10)), trees(range = c(1,50)), levels= 5)

tune_bag <- tune_grid(
  bagging_wf, 
  resamples = wine_folds, 
  grid = param_gridbag
)

autoplot(tune_bag)
show_best(tune_bag)
```

I decided to explore the bagged model more closely, despite it being included in the forest model above. Note mtry is set to 11, since that is the maximum number of predictors. Trees and min_n are still tuned, though. It appears the higher number of trees, the better performance, but the lower minimal node is better, excluding min_n = 1. The best model has 50 trees and a minimal node of 3. It yields an roc_auc value of 0.746 and an error of 0.00968, so it performed slightly worse than the forest model. 

## Boosted Tree Model
```{r collapse=TRUE, message=FALSE, warning=FALSE}
boost_mod <- boost_tree(trees = tune(), tree_depth = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

boost_wf <- workflow() %>%
  add_model(boost_mod) %>%
  add_recipe(wine_recipe)

param_gridb <- grid_regular(tree_depth(range = c(1,10)), trees(range = c(2500,10000)), levels= 5)

tune_boost <- tune_grid(
  boost_wf, 
  resamples = wine_folds, 
  grid = param_gridb 
)

autoplot(tune_boost)
show_best(tune_boost)
```

The boosted tree model is the final one I decided to fit. I decided to tune the number of trees, as well as tree depth.  It looks like the lower number of depth yields better performance. The higher number of trees yields a lower performance. The best boosted tree model has 2500 trees, a tree depth of 1, an roc_auc value of 0.730, and a standard error of 0.00847. This is a worse performance than the normal forest.

```{r collapse = TRUE}
naive_acc <- collect_metrics(naive_fit)[2,]
boost_acc <- show_best(tune_bag, metric = "roc_auc")[1,]
bagging_acc <- show_best(tune_boost, metric = "roc_auc")[1,]
lsvm_acc <- show_best(svm_tune, metric = "roc_auc")[1,]
nlsvm_acc <- show_best(svm_tunen, metric = "roc_auc")[1,]
knn_acc <- show_best(knn_tune, metric = "roc_auc")[1,]
forest_acc <- show_best(tune_resf, metric = "roc_auc")[1,]

model_metrics <- bind_rows(naive_acc, lsvm_acc, nlsvm_acc, knn_acc, forest_acc, bagging_acc, boost_acc) %>% 
  mutate(.config = c("NB", "LSVM", "NLSVM", "KNN", "Forest", "Bagging", "Boost"))%>%
  tibble() %>%
  arrange(desc(mean))

model_metrics
```

# Model Selection and Performance

Our Forest model performed the best on the training data, given all the models roc_auc values. Now, we will fit our forest model to the testing data and see how it performs.

```{r collapse=TRUE, message=FALSE, warning=FALSE}
best_forest <- select_best(tune_resf, metric = "roc_auc")

forest_final_wf <- finalize_workflow(forest_wf, best_forest)

wf_final <- fit(forest_final_wf, data = wine_train)

augment(wf_final, new_data = wine_test) %>%
  roc_auc(quality, 3:8)

augment(wf_final, new_data = wine_test) %>%
  roc_curve(droplevels(quality), 4:8) %>% autoplot()

augment(wf_final, new_data = wine_test) %>%
conf_mat(quality, .pred_class) %>%
  autoplot(type = "heatmap")

wf_final %>%
  extract_fit_parsnip() %>%
  vip()
```

After fitting our model to the testing data, it yielded an roc_auc value of 0.491. From the variable importance plot, we see alcohol, sulphates, and volatile acid were the most important and citric acid, chlorides, and free sulfur dioxide being the least important. Note that the wine testing set does not contain quality of level 3, so I decided to omit it when calculating the roc curves. From the roc curves and confusion matrix, I see that my model was best at predicting wine of quality 5 or 6 and was worst at calculating wine of quality 7 and 8. It also appears, generally, that my model predicts the quality is lower by one than the truth. Overall, though, it did pretty well as most predictions were only off by 1, so if it was a regression model, the accuracy may have been higher. 

# Conclusion 

It appears that the Forest, Non-Linear SVM, and bagging models performed the best overall, and the Linear SVM, Naive Bayes, and K-Nearest Neighbors models performing the worst overall. Our Forest model performed significantly better on the training data though, with an roc_auc value of 0.782. Note the tuning parameter values are mtry = 3, trees = 50, and min_n = 10. I was surprised that alcohol content was the most important variable determining quality and that p_h was not as important. I am also surprised my boosted tree model did not perform better than my bagged model since our bagged model parameter mtry was constant. My next steps would be to tune the model further and modify the recipe to yield a higher value of accuracy. In order to better predict quality, I would recommend going forward subsetting the quality levels and determining why one level was predicted at a higher accuracy than another. 